{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7307edb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import os\n",
    "import cv2\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from absl import app, flags, logging\n",
    "from absl.flags import FLAGS\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import random\n",
    "import tensorflow as tf\n",
    "### configs path  \n",
    "cfg_path= './configs/esrgan.yaml'\n",
    "gpu= '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a27fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def main():\n",
    "    folder = './data/DIV2K_train_LR_bicubic_X4/DIV2K_train_LR_bicubic/X4'\n",
    "    DIV2K(folder)\n",
    "    print('Finished.')\n",
    "\n",
    "\n",
    "def DIV2K(path):\n",
    "    img_path_l = glob.glob(os.path.join(path, '*'))\n",
    "    for img_path in img_path_l:\n",
    "        new_path = img_path.replace('x2', '').replace('x3', '').replace(\n",
    "            'x4', '').replace('x8', '')\n",
    "        os.rename(img_path, new_path)\n",
    "        \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1d4abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses function\n",
    "def PixelLoss(criterion='l1'):\n",
    "    \"\"\"pixel loss\"\"\"\n",
    "    if criterion == 'l1':\n",
    "        return tf.keras.losses.MeanAbsoluteError()\n",
    "    elif criterion == 'l2':\n",
    "        return tf.keras.losses.MeanSquaredError()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Loss type {} is not recognized.'.format(criterion))\n",
    "\n",
    "\n",
    "def ContentLoss(criterion='l1', output_layer=54, before_act=True):\n",
    "    \"\"\"content loss\"\"\"\n",
    "    if criterion == 'l1':\n",
    "        loss_func = tf.keras.losses.MeanAbsoluteError()\n",
    "    elif criterion == 'l2':\n",
    "        loss_func = tf.keras.losses.MeanSquaredError()\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Loss type {} is not recognized.'.format(criterion))\n",
    "    vgg = VGG19(input_shape=(None, None, 3), include_top=False)\n",
    "\n",
    "    if output_layer == 22:  # Low level feature\n",
    "        pick_layer = 5\n",
    "    elif output_layer == 54:  # Hight level feature\n",
    "        pick_layer = 20\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'VGG output layer {} is not recognized.'.format(criterion))\n",
    "\n",
    "    if before_act:\n",
    "        vgg.layers[pick_layer].activation = None\n",
    "\n",
    "    fea_extrator = tf.keras.Model(vgg.input, vgg.layers[pick_layer].output)\n",
    "\n",
    "    @tf.function\n",
    "    def content_loss(hr, sr):\n",
    "        # the input scale range is [0, 1] (vgg is [0, 255]).\n",
    "        # 12.75 is rescale factor for vgg featuremaps.\n",
    "        preprocess_sr = preprocess_input(sr * 255.) / 12.75\n",
    "        preprocess_hr = preprocess_input(hr * 255.) / 12.75\n",
    "        sr_features = fea_extrator(preprocess_sr)\n",
    "        hr_features = fea_extrator(preprocess_hr)\n",
    "\n",
    "        return loss_func(hr_features, sr_features)\n",
    "\n",
    "    return content_loss\n",
    "\n",
    "\n",
    "def DiscriminatorLoss(gan_type='ragan'):\n",
    "    \"\"\"discriminator loss\"\"\"\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    sigma = tf.sigmoid\n",
    "\n",
    "    def discriminator_loss_ragan(hr, sr):\n",
    "        return 0.5 * (\n",
    "            cross_entropy(tf.ones_like(hr), sigma(hr - tf.reduce_mean(sr))) +\n",
    "            cross_entropy(tf.zeros_like(sr), sigma(sr - tf.reduce_mean(hr))))\n",
    "\n",
    "    def discriminator_loss(hr, sr):\n",
    "        real_loss = cross_entropy(tf.ones_like(hr), sigma(hr))\n",
    "        fake_loss = cross_entropy(tf.zeros_like(sr), sigma(sr))\n",
    "        return real_loss + fake_loss\n",
    "\n",
    "    if gan_type == 'ragan':\n",
    "        return discriminator_loss_ragan\n",
    "    elif gan_type == 'gan':\n",
    "        return discriminator_loss\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Discriminator loss type {} is not recognized.'.format(gan_type))\n",
    "\n",
    "\n",
    "def GeneratorLoss(gan_type='ragan'):\n",
    "    \"\"\"generator loss\"\"\"\n",
    "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    sigma = tf.sigmoid\n",
    "\n",
    "    def generator_loss_ragan(hr, sr):\n",
    "        return 0.5 * (\n",
    "            cross_entropy(tf.ones_like(sr), sigma(sr - tf.reduce_mean(hr))) +\n",
    "            cross_entropy(tf.zeros_like(hr), sigma(hr - tf.reduce_mean(sr))))\n",
    "\n",
    "    def generator_loss(hr, sr):\n",
    "        return cross_entropy(tf.ones_like(sr), sigma(sr))\n",
    "\n",
    "    if gan_type == 'ragan':\n",
    "        return generator_loss_ragan\n",
    "    elif gan_type == 'gan':\n",
    "        return generator_loss\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            'Generator loss type {} is not recognized.'.format(gan_type))\n",
    "#learing rate\n",
    "def MultiStepLR(initial_learning_rate, lr_steps, lr_rate, name='MultiStepLR'):\n",
    "    \"\"\"Multi-steps learning rate scheduler.\"\"\"\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value)\n",
    "\n",
    "\n",
    "def CosineAnnealingLR_Restart(initial_learning_rate, t_period, lr_min):\n",
    "    \"\"\"Cosine annealing learning rate scheduler with restart.\"\"\"\n",
    "    return tf.keras.experimental.CosineDecayRestarts(\n",
    "        initial_learning_rate=initial_learning_rate,\n",
    "        first_decay_steps=t_period, t_mul=1.0, m_mul=1.0,\n",
    "        alpha=lr_min / initial_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216054c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_dataset_path='./data/DIV2K_train_HR/DIV2K_train_HR/' # 'path to high resolution dataset'\n",
    "lr_dataset_path='./data/DIV2K_train_LR_bicubic_X4/DIV2K_train_LR_bicubic/X4/'#'path to low resolution dataset')\n",
    "output_path='./data/DIV2K800_sub_bin.tfrecord'#'path to ouput tfrecord')\n",
    "is_binary=True #'whether save images as binary files'' or load them on the fly.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy()\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def make_example_bin(img_name, hr_img_str, lr_img_str):\n",
    "    # Create a dictionary with features that may be relevant (binary).\n",
    "    feature = {'image/img_name': _bytes_feature(img_name),\n",
    "               'image/hr_encoded': _bytes_feature(hr_img_str),\n",
    "               'image/lr_encoded': _bytes_feature(lr_img_str)}\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def make_example(img_name, hr_img_path, lr_img_path):\n",
    "    # Create a dictionary with features that may be relevant.\n",
    "    feature = {'image/img_name': _bytes_feature(img_name),\n",
    "               'image/hr_img_path': _bytes_feature(hr_img_path),\n",
    "               'image/lr_img_path': _bytes_feature(lr_img_path)}\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    if not os.path.isdir(hr_dataset_path):\n",
    "        logging.info('Please define valid dataset path.')\n",
    "    else:\n",
    "        logging.info('Loading {}'.format(hr_dataset_path))\n",
    "\n",
    "    samples = []\n",
    "    logging.info('Reading data list...')\n",
    "    for hr_img_path in glob.glob(os.path.join(hr_dataset_path, '*.png')):\n",
    "        img_name = os.path.basename(hr_img_path).replace('.png', '')\n",
    "        lr_img_path = os.path.join(lr_dataset_path, img_name + '.png')\n",
    "        samples.append((img_name, hr_img_path, lr_img_path))\n",
    "    random.shuffle(samples)\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        logging.info('{:s} already exists. Exit...'.format(output_path))\n",
    "        exit(1)\n",
    "\n",
    "    logging.info('Writing {} sample to tfrecord file...'.format(len(samples)))\n",
    "    with tf.io.TFRecordWriter(output_path) as writer:\n",
    "        for img_name, hr_img_path, lr_img_path in tqdm.tqdm(samples):\n",
    "            if is_binary:\n",
    "                hr_img_str = open(hr_img_path, 'rb').read()\n",
    "                lr_img_str = open(lr_img_path, 'rb').read()\n",
    "                tf_example = make_example_bin(img_name=str.encode(img_name),\n",
    "                                              hr_img_str=hr_img_str,\n",
    "                                              lr_img_str=lr_img_str)\n",
    "            else:\n",
    "                tf_example = make_example(img_name=str.encode(img_name),\n",
    "                                          hr_img_path=str.encode(hr_img_path),\n",
    "                                          lr_img_path=str.encode(lr_img_path))\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "            \n",
    "main()\n",
    "\n",
    "\n",
    "# ## End of dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpu\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "set_memory_growth()\n",
    "\n",
    "cfg = load_yaml(cfg_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator and discriminat model \n",
    "\n",
    "\n",
    "def _regularizer(weights_decay=5e-4):\n",
    "    return tf.keras.regularizers.l2(weights_decay)\n",
    "\n",
    "\n",
    "def _kernel_init(scale=1.0, seed=None):\n",
    "    \"\"\"He normal initializer with scale.\"\"\"\n",
    "    scale = 2. * scale\n",
    "    return tf.keras.initializers.VarianceScaling(\n",
    "        scale=scale, mode='fan_in', distribution=\"truncated_normal\", seed=seed)\n",
    "\n",
    "\n",
    "class BatchNormalization(tf.keras.layers.BatchNormalization):\n",
    "    \"\"\"Make trainable=False freeze BN for real (the og version is sad).\n",
    "       ref: https://github.com/zzh8829/yolov3-tf2\n",
    "    \"\"\"\n",
    "    def __init__(self, axis=-1, momentum=0.9, epsilon=1e-5, center=True,\n",
    "                 scale=True, name=None, **kwargs):\n",
    "        super(BatchNormalization, self).__init__(\n",
    "            axis=axis, momentum=momentum, epsilon=epsilon, center=center,\n",
    "            scale=scale, name=name, **kwargs)\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        if training is None:\n",
    "            training = tf.constant(False)\n",
    "        training = tf.logical_and(training, self.trainable)\n",
    "        return super().call(x, training)\n",
    "\n",
    "\n",
    "class ResDenseBlock_5C(tf.keras.layers.Layer):\n",
    "    \"\"\"Residual Dense Block\"\"\"\n",
    "    def __init__(self, nf=64, gc=32, res_beta=0.2, wd=0., name='RDB5C',\n",
    "                 **kwargs):\n",
    "        super(ResDenseBlock_5C, self).__init__(name=name, **kwargs)\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.res_beta = res_beta\n",
    "        lrelu_f = functools.partial(LeakyReLU, alpha=0.2)\n",
    "        _Conv2DLayer = functools.partial(\n",
    "            Conv2D, kernel_size=3, padding='same',\n",
    "            kernel_initializer=_kernel_init(0.1), bias_initializer='zeros',\n",
    "            kernel_regularizer=_regularizer(wd))\n",
    "        self.conv1 = _Conv2DLayer(filters=gc, activation=lrelu_f())\n",
    "        self.conv2 = _Conv2DLayer(filters=gc, activation=lrelu_f())\n",
    "        self.conv3 = _Conv2DLayer(filters=gc, activation=lrelu_f())\n",
    "        self.conv4 = _Conv2DLayer(filters=gc, activation=lrelu_f())\n",
    "        self.conv5 = _Conv2DLayer(filters=nf, activation=lrelu_f())\n",
    "\n",
    "    def call(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(tf.concat([x, x1], 3))\n",
    "        x3 = self.conv3(tf.concat([x, x1, x2], 3))\n",
    "        x4 = self.conv4(tf.concat([x, x1, x2, x3], 3))\n",
    "        x5 = self.conv5(tf.concat([x, x1, x2, x3, x4], 3))\n",
    "        return x5 * self.res_beta + x\n",
    "\n",
    "\n",
    "class ResInResDenseBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Residual in Residual Dense Block\"\"\"\n",
    "    def __init__(self, nf=64, gc=32, res_beta=0.2, wd=0., name='RRDB',\n",
    "                 **kwargs):\n",
    "        super(ResInResDenseBlock, self).__init__(name=name, **kwargs)\n",
    "        self.res_beta = res_beta\n",
    "        self.rdb_1 = ResDenseBlock_5C(nf, gc, res_beta=res_beta, wd=wd)\n",
    "        self.rdb_2 = ResDenseBlock_5C(nf, gc, res_beta=res_beta, wd=wd)\n",
    "        self.rdb_3 = ResDenseBlock_5C(nf, gc, res_beta=res_beta, wd=wd)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.rdb_1(x)\n",
    "        out = self.rdb_2(out)\n",
    "        out = self.rdb_3(out)\n",
    "        return out * self.res_beta + x\n",
    "\n",
    "\n",
    "def RRDB_Model(size, channels, cfg_net, gc=32, wd=0., name='RRDB_model'):\n",
    "    \"\"\"Residual-in-Residual Dense Block based Model \"\"\"\n",
    "    nf, nb = cfg_net['nf'], cfg_net['nb']\n",
    "    lrelu_f = functools.partial(LeakyReLU, alpha=0.2)\n",
    "    rrdb_f = functools.partial(ResInResDenseBlock, nf=nf, gc=gc, wd=wd)\n",
    "    conv_f = functools.partial(Conv2D, kernel_size=3, padding='same',\n",
    "                               bias_initializer='zeros',\n",
    "                               kernel_initializer=_kernel_init(),\n",
    "                               kernel_regularizer=_regularizer(wd))\n",
    "    rrdb_truck_f = tf.keras.Sequential(\n",
    "        [rrdb_f(name=\"RRDB_{}\".format(i)) for i in range(nb)],\n",
    "        name='RRDB_trunk')\n",
    "\n",
    "    # extraction\n",
    "    x = inputs = Input([size, size, channels], name='input_image')\n",
    "    fea = conv_f(filters=nf, name='conv_first')(x)\n",
    "    fea_rrdb = rrdb_truck_f(fea)\n",
    "    trunck = conv_f(filters=nf, name='conv_trunk')(fea_rrdb)\n",
    "    fea = fea + trunck\n",
    "\n",
    "    # upsampling\n",
    "    size_fea_h = tf.shape(fea)[1] if size is None else size\n",
    "    size_fea_w = tf.shape(fea)[2] if size is None else size\n",
    "    fea_resize = tf.image.resize(fea, [size_fea_h * 2, size_fea_w * 2],\n",
    "                                 method='nearest', name='upsample_nn_1')\n",
    "    fea = conv_f(filters=nf, activation=lrelu_f(), name='upconv_1')(fea_resize)\n",
    "    fea_resize = tf.image.resize(fea, [size_fea_h * 4, size_fea_w * 4],\n",
    "                                 method='nearest', name='upsample_nn_2')\n",
    "    fea = conv_f(filters=nf, activation=lrelu_f(), name='upconv_2')(fea_resize)\n",
    "    fea = conv_f(filters=nf, activation=lrelu_f(), name='conv_hr')(fea)\n",
    "    out = conv_f(filters=channels, name='conv_last')(fea)\n",
    "\n",
    "    return Model(inputs, out, name=name)\n",
    "\n",
    "\n",
    "def DiscriminatorVGG128(size, channels, nf=64, wd=0.,\n",
    "                        name='Discriminator_VGG_128'):\n",
    "    \"\"\"Discriminator VGG 128\"\"\"\n",
    "    lrelu_f = functools.partial(LeakyReLU, alpha=0.2)\n",
    "    conv_k3s1_f = functools.partial(Conv2D,\n",
    "                                    kernel_size=3, strides=1, padding='same',\n",
    "                                    kernel_initializer=_kernel_init(),\n",
    "                                    kernel_regularizer=_regularizer(wd))\n",
    "    conv_k4s2_f = functools.partial(Conv2D,\n",
    "                                    kernel_size=4, strides=2, padding='same',\n",
    "                                    kernel_initializer=_kernel_init(),\n",
    "                                    kernel_regularizer=_regularizer(wd))\n",
    "    dese_f = functools.partial(Dense, kernel_regularizer=_regularizer(wd))\n",
    "\n",
    "    x = inputs = Input(shape=(size, size, channels))\n",
    "\n",
    "    x = conv_k3s1_f(filters=nf, name='conv0_0')(x)\n",
    "    x = conv_k4s2_f(filters=nf, use_bias=False, name='conv0_1')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn0_1')(x))\n",
    "\n",
    "    x = conv_k3s1_f(filters=nf * 2, use_bias=False, name='conv1_0')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn1_0')(x))\n",
    "    x = conv_k4s2_f(filters=nf * 2, use_bias=False, name='conv1_1')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn1_1')(x))\n",
    "\n",
    "    x = conv_k3s1_f(filters=nf * 4, use_bias=False, name='conv2_0')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn2_0')(x))\n",
    "    x = conv_k4s2_f(filters=nf * 4, use_bias=False, name='conv2_1')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn2_1')(x))\n",
    "\n",
    "    x = conv_k3s1_f(filters=nf * 8, use_bias=False, name='conv3_0')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn3_0')(x))\n",
    "    x = conv_k4s2_f(filters=nf * 8, use_bias=False, name='conv3_1')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn3_1')(x))\n",
    "\n",
    "    x = conv_k3s1_f(filters=nf * 8, use_bias=False, name='conv4_0')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn4_0')(x))\n",
    "    x = conv_k4s2_f(filters=nf * 8, use_bias=False, name='conv4_1')(x)\n",
    "    x = lrelu_f()(BatchNormalization(name='bn4_1')(x))\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = dese_f(units=100, activation=lrelu_f(), name='linear1')(x)\n",
    "    out = dese_f(units=1, name='linear2')(x)\n",
    "\n",
    "    return Model(inputs, out, name=name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403b6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Generator \n",
    "generator = RRDB_Model(cfg['input_size'], cfg['ch_size'], cfg['network_G'])\n",
    "generator.summary(line_length=80)\n",
    "### Discriminator\n",
    "discriminator = DiscriminatorVGG128(cfg['gt_size'], cfg['ch_size'])\n",
    "discriminator.summary(line_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "train_dataset = load_dataset(cfg, 'train_dataset', shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7704180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "define optimizers\n",
    "learning_rate_G = MultiStepLR(cfg['lr_G'], cfg['lr_steps'], cfg['lr_rate'])\n",
    "learning_rate_D = MultiStepLR(cfg['lr_D'], cfg['lr_steps'], cfg['lr_rate'])\n",
    "optimizer_G = tf.keras.optimizers.Adam(learning_rate=learning_rate_G,beta_1=cfg['adam_beta1_G'],beta_2=cfg['adam_beta2_G'])\n",
    "optimizer_D = tf.keras.optimizers.Adam(learning_rate=learning_rate_D,beta_1=cfg['adam_beta1_D'],beta_2=cfg['adam_beta2_D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses function\n",
    "pixel_loss_fn = PixelLoss(criterion=cfg['pixel_criterion'])\n",
    "fea_loss_fn = ContentLoss(criterion=cfg['feature_criterion'])\n",
    "gen_loss_fn = GeneratorLoss(gan_type=cfg['gan_type'])\n",
    "dis_loss_fn = DiscriminatorLoss(gan_type=cfg['gan_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39828df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## training step function\n",
    "def train_step(lr, hr):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        sr = generator(lr, training=True)\n",
    "        hr_output = discriminator(hr, training=True)\n",
    "        sr_output = discriminator(sr, training=True)\n",
    "\n",
    "        losses_G = {}\n",
    "        losses_D = {}\n",
    "        losses_G['reg'] = tf.reduce_sum(generator.losses)\n",
    "        losses_D['reg'] = tf.reduce_sum(discriminator.losses)\n",
    "        losses_G['pixel'] = cfg['w_pixel'] * pixel_loss_fn(hr, sr)\n",
    "        losses_G['feature'] = cfg['w_feature'] * fea_loss_fn(hr, sr)\n",
    "        losses_G['gan'] = cfg['w_gan'] * gen_loss_fn(hr_output, sr_output)\n",
    "        losses_D['gan'] = dis_loss_fn(hr_output, sr_output)\n",
    "        total_loss_G = tf.add_n([l for l in losses_G.values()])\n",
    "        total_loss_D = tf.add_n([l for l in losses_D.values()])\n",
    "\n",
    "    grads_G = tape.gradient(\n",
    "        total_loss_G, generator.trainable_variables)\n",
    "    grads_D = tape.gradient(\n",
    "        total_loss_D, discriminator.trainable_variables)\n",
    "    optimizer_G.apply_gradients(\n",
    "        zip(grads_G, generator.trainable_variables))\n",
    "    optimizer_D.apply_gradients(\n",
    "        zip(grads_D, discriminator.trainable_variables))\n",
    "\n",
    "    return total_loss_G, total_loss_D, losses_G, losses_D\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c217579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint\n",
    "checkpoint_dir = './esrgan_inference' \n",
    "checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n",
    "                                 optimizer_G=optimizer_G,\n",
    "                                 optimizer_D=optimizer_D,\n",
    "                                 model=generator,\n",
    "                                 discriminator=discriminator)\n",
    "manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                     directory=checkpoint_dir,\n",
    "                                     max_to_keep=3)\n",
    "if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print('[*] load ckpt from {} at step {}.'.format(\n",
    "            manager.latest_checkpoint, checkpoint.step.numpy()))\n",
    "else:\n",
    "    if cfg['pretrain_name'] is not None:\n",
    "        pretrain_dir = './checkpoints/' + cfg['pretrain_name']\n",
    "        if tf.train.latest_checkpoint(pretrain_dir):\n",
    "            checkpoint.restore(tf.train.latest_checkpoint(pretrain_dir))\n",
    "            checkpoint.step.assign(0)\n",
    "            print(\"[*] training from pretrain model {}.\".format(\n",
    "                pretrain_dir))\n",
    "        else:\n",
    "            print(\"[*] cannot find pretrain model {}.\".format(\n",
    "                pretrain_dir))\n",
    "    else:\n",
    "        print(\"[*] training from scratch.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
